{"componentChunkName":"component---src-templates-blog-post-js","path":"/context-engineering-with-claude-code-my-evolving-workflow/","result":{"data":{"site":{"siteMetadata":{"title":"Alabê's Blog"}},"markdownRemark":{"id":"5c2d4b69-66c2-5a4f-9c50-1203e2168f8a","excerpt":"Context engineering with AI assistants is evolving rapidly, and everyone seems to have their own approach. This article shares a workflow I’ve been…","html":"<p>Context engineering with AI assistants is evolving rapidly, and everyone seems to have their own approach. This article shares a workflow I’ve been experimenting with over a couple of months using Claude Code - a practical method for maintaining context across sessions, validating understanding, and turning AI collaboration into well-documented, testable code.</p>\n<p>There are heaps of excellent resources on context engineering out there - <a href=\"https://www.anthropic.com/engineering/claude-code-best-practices\">Claude Code Best Practices</a> from Anthropic, <a href=\"https://youtu.be/IS_y40zY-hc?si=tgSP87N5SYlrMDnD\">Simon Willison’s exploration of Claude Code</a>, and <a href=\"https://www.youtube.com/watch?v=fHWFF_pnqDk\">Ethan Mollick’s deep dive into working with AI tools</a> to name a few. Every day brings new articles with tips and tricks for using AI assistants effectively - it can be overwhelming, even repetitive at times. Some are genuinely insightful, others feel like variations on the same theme. What I want to share here is something different: my personal journey over the last few months. Not another prescriptive guide, but the workflow that’s actually stuck and made a tangible difference to how I write code.</p>\n<p>I won’t give you prescriptive prompts or a <code class=\"language-text\">CLAUDE.md</code> file to copy and paste. Instead, I want to explain the workflow and reasoning behind it, so you can adapt it to your own context.</p>\n<h2>Building Context Through Dialogue</h2>\n<p>Working with large codebases presents a unique challenge: features rarely live in isolation. They span multiple files, interact with various systems, and carry implicit knowledge that’s often scattered across team members’ heads and outdated documentation.</p>\n<p>What I found that worked great was starting with a simple premise: I already understand the code I’m working with, but I want to validate that understanding and create a shared context with Claude Code.</p>\n<p>Here’s an example of how this works. I start by describing what I know about how a feature works in my own words. I’m not asking Claude to explain the code to me - I’m explaining it myself and asking for validation. I’ll typically mention an entry point where the logic begins, something like: “The authentication flow starts in <code class=\"language-text\">auth/middleware.go</code> and I believe it checks the JWT token, validates it against our auth service, and then either proceeds or redirects to login.”</p>\n<p>Claude Code then gives me its understanding based on the actual code. This is where the magic happens - it’s not about getting an explanation, it’s about calibrating our shared understanding. If there’s a mismatch, I’ll follow up with corrections or clarifications until we’re aligned.</p>\n<p>Once I’m happy with our shared understanding, I ask Claude Code to write this down in a markdown file - let’s call it <code class=\"language-text\">AUTHENTICATION_FLOW.md</code>. Now I have a structured, written form of how this particular feature works. Remember, I’m documenting a specific feature, not the entire codebase. This focused approach keeps things manageable and relevant.</p>\n<h2>From Understanding to Planning</h2>\n<p>With our context documented, I start fresh with a <code class=\"language-text\">/clear</code> command. This clean slate is intentional - it forces me to be explicit about what context is needed for the next phase.</p>\n<p>Now I can say: “Based on @AUTHENTICATION_FLOW.md, I want to add rate limiting per user, implement refresh token rotation, and add device fingerprinting.”</p>\n<p>But here’s where I add my own thinking to the mix. I’ll usually have ideas about implementation approaches, so I’ll include them: “I thought about using Redis for rate limiting and I think the trade-off is additional infrastructure complexity but better performance at scale. Alternatively, I could use in-memory rate limiting which is simpler but won’t work across multiple instances. I need you to weigh in on these approaches and come up with an implementation plan.”</p>\n<p>I do this in plan mode (rotate shift + tab in Claude Code), which helps maintain focus on planning rather than jumping straight to implementation. The back-and-forth here is crucial - I challenge Claude Code with questions, explore edge cases, and refine until I’m confident we have not only a direction I’m happy with but also documented reasoning about alternatives considered. This becomes invaluable later when I need to write design documents or when revisiting decisions months down the track.</p>\n<h3>Iterative Planning and Documentation</h3>\n<p>Claude Code tends to create plans with multiple phases, and I often tweak these. Sometimes it over-engineers things, so I’ll consolidate or remove phases. This granular control helps break down complex problems into manageable chunks, and I can git commit after each successful phase implementation.</p>\n<p>Depending on the complexity, I’ll either append to the existing markdown file or create a new one for the plan. The key is maintaining that written record of decisions and approach.</p>\n<h3>Using Tests as Validation Checkpoints</h3>\n<p>Here’s a critical addition to my workflow that’s saved me countless hours: I ask Claude Code to write tests first, then use them as validation checkpoints. Instead of me manually checking if the implementation works, Claude runs the tests to verify correctness.</p>\n<p>This TDD approach does consume roughly 2-3x more tokens due to iterative test-fail-implement-pass cycles and context accumulation. The <a href=\"https://cybersecurity.springeropen.com/articles/10.1186/s42400-024-00335-4\">LLM4TDG framework research</a> found that Test Driven Refactoring modules increased average token consumption by 181.21% and test case generation time by 207.36% compared to non-TDD approaches. <a href=\"https://blog.scottlogic.com/2023/12/18/implementing-cost-effective-test-driven-development-in-an-llm-application.html\">Scott Logic’s analysis</a> found that TDD with LLMs involves multiple test runs and evaluation LLMs, effectively doubling token usage for testing purposes.</p>\n<p>But here’s the thing - I’d write tests anyway, with or without AI. Tests are living documentation that verify the code works as intended. While code remains the source of truth, tests ensure that truth aligns with expectations. The token investment essentially automates what I’d do manually, with the added benefit of reduced debugging cycles and higher confidence in the implementation. Despite higher token usage, <a href=\"https://arxiv.org/abs/2402.13521\">this arXiv preprint on Test-Driven Development for Code Generation</a> shows TDD with AI assistants leads to higher success in solving programming challenges and provides concrete specifications that reduce ambiguity for LLMs.</p>\n<p>This creates a powerful feedback loop. The tests become the specification, and Claude Code iterates on the implementation until they pass.</p>\n<h2>Implementation with Precision</h2>\n<p>Time for another <code class=\"language-text\">/clear</code> - starting fresh for the implementation phase. This separation between planning and implementation has been crucial for keeping the context manageable.</p>\n<p>My editor of choice is Neovim with the <a href=\"https://github.com/coder/claudecode.nvim\">claudecode.nvim plugin</a>, which has been particularly helpful for maintaining focus. Instead of saying “implement phase 2”, I can select specific lines from the plan and send them directly with references like <code class=\"language-text\">@CACHING_ENHANCEMENT_PLAN.md#L7-17</code>.</p>\n<p>This approach has helped me explore code more effectively. When I need to understand a particular section, I can select it and ask targeted questions about its behaviour or edge cases. Being able to provide precise context has made these conversations much more productive.</p>\n<p>Most of the time, I prefer to handle the git workflow myself - staging files as chunks of work are completed so I know exactly what’s being added. But I let Claude Code write the commit messages. It’s genuinely excellent at understanding what was done and writing comprehensive commit messages that capture both the what and the why.</p>\n<p>This also helps with continuity. When I return to a feature hours later, instead of providing the entire plan again, I can just give Claude Code a couple of recent commits to read and it picks up right where we left off.</p>\n<h2>Conclusion</h2>\n<p>My development workflow is evolving, and I’m still figuring it out. The approach I’ve described here is what’s been working for me lately, but it’s constantly changing as I try to keep up with how quickly these tools are improving.</p>\n<p>The biggest shift for me has been becoming more pragmatic. I still care deeply about clean APIs, good abstractions, and code that expresses clear intent - sometimes I find myself arguing with Claude about these things. But now that AI generates code for me, I’m learning to pick my battles. The craft still matters, but I’m less precious about being the one who types it all out.</p>\n<p>Building context through documentation has become central to how I work, though I’m not committing these context files to git yet. These artifacts are still personal tools - they help me maintain continuity between sessions and occasionally help when explaining my thinking to teammates. But there’s a journey ahead in figuring out how this fits with how everyone else on the team works.</p>\n<p>This way of working is still evolving. The tools keep changing, my approach keeps adapting, and I’m still discovering what works and what doesn’t. But for now, this structured context-building has shifted development from wrestling with complexity alone to having a conversation about solving problems - even if sometimes that conversation involves me shouting at Claude about proper error handling.</p>","frontmatter":{"title":"Context Engineering: How I've Been Using Claude Code in My Development Workflow","date":"September 13, 2025","description":"A practical method for maintaining context across sessions, validating understanding, and turning AI collaboration into well-documented, testable code."}},"previous":{"fields":{"slug":"/retrospectives/roles-and-expectations/"},"frontmatter":{"title":"Retrospectives - Roles and Expectations"}},"next":null},"pageContext":{"id":"5c2d4b69-66c2-5a4f-9c50-1203e2168f8a","previousPostId":"43585348-5a72-5a03-ac08-01d491a8cef0","nextPostId":null}},"staticQueryHashes":["2355076697","3347749741"],"slicesMap":{}}